![IaC](https://img.shields.io/badge/IaC-Databricks%20Asset%20Bundles-8B5CF6?style=flat-square&logo=terraform&logoColor=white)
![Azure DevOps](https://img.shields.io/badge/CI%2FCD-Azure%20DevOps-2563eb?style=flat-square&logo=azuredevops&logoColor=white)
![DLT](https://img.shields.io/badge/Delta%20Live%20Tables-DLT-059669?style=flat-square)
![Unity Catalog](https://img.shields.io/badge/Unity%20Catalog-Ready-6b7280?style=flat-square)
![Security](https://img.shields.io/badge/Security-Service%20Principal-DC2626?style=flat-square)
![License](https://img.shields.io/badge/License-MIT-111827?style=flat-square&logo=open-source-initiative&logoColor=white)
![Author](https://img.shields.io/badge/Kishore%20Kumar%20Nagisetty-Maintainer-ec4899?style=flat-square&logo=github&logoColor=white)



## ğŸ“¦ Features

- ğŸ§± **Declarative IaC**: Jobs & DLT pipelines modelled in YAML under `resources/`.
- ğŸ” **Environment Promotion**: Single artifact built once, promoted across **DEV â†’ TEST â†’ PROD**.
- ğŸ” **Run-as SPN**: All deployments use Azure AD **Service Principal** (non-interactive & secure).
- âœ… **Validation**: `databricks bundle validate` in build; `bundle deploy` in each environment.
- ğŸ§¾ **Unity Catalog Ready**: Catalog & schema injected via variables.
- âš™ï¸ **Autoscaling Clusters**: Node types and min/max workers parameterised.

---

## ğŸ“ Repository Structure
```bash
.
â”œâ”€â”€ resources
â”‚   â”œâ”€â”€ jobs
â”‚   â”‚   â”œâ”€â”€ Absence Data.yaml              # Job: Notebook task â†’ DLT task
â”‚   â”‚   â””â”€â”€ Absence Spells Test.yaml
â”‚   â””â”€â”€ pipelines
â”‚       â”œâ”€â”€ Absence Data DLT.yaml          # DLT pipeline config
â”‚       â””â”€â”€ Absence Spells Test DLT.yaml
â”œâ”€â”€ templates
â”‚   â”œâ”€â”€ main.yaml                          # Build â†’ DeployToDev â†’ DeployToTest
â”‚   â””â”€â”€ deployment.yaml                    # Generic deploy (param: buildEnv)
â”œâ”€â”€ databricks.yml                         # DAB bundle & targets (env vars injected)
â”œâ”€â”€ fixtures/.gitkeep                      # Example snippet for loading CSV fixtures (docs)
â””â”€â”€ README.md

âš™ï¸ How It Works
Build (AzDO)
Install Databricks CLI.
Replace placeholders inside databricks.yml with env values from Variable Groups.
databricks bundle validate -t <ENV> to sanity check.
Publish DatabricksBundle artifact.
Deploy (AzDO)
Download artifact and re-inject env values (DEV/TEST/PROD).
az login with Service Principal.
databricks bundle deploy -t <ENV> to create/update Jobs & DLT Pipelines.
Already have objects in the workspace? Bind once
databricks bundle deployment bind -t <env> <resource-name> <existing-id> --auto-approve

Prerequisites
âœ… Azure Databricks workspaces for DEV/TEST/PROD
âœ… Azure AD Service Principal with workspace access
âœ… Azure DevOps Project + Variable Groups per env:
Dab_DEV_Dbw_Variables, Dab_Test_Dbw_Variables, â€¦
âœ… Variables defined (per env):
workspaceUrl, deployEnv, prefixNotebookPath, storageAccount, container, folderPath, existingClusterId, DLTClusterNodeType, DLTMinWorkers, DLTMaxWorkers, DLTSchema, catalog, successEmailDL, failureEmailDL, clientid, clientsecret, tenantid
